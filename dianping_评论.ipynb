{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入菜单序号： 1. 爬页面  2. 解析 html 文件  3. 抓取未抓取到的页面: 1\n",
      "请输入要抓取的页面数量（如要抓取100页，则输入100）：10\n",
      "正在抓取第[1]张页面\n",
      "第[1]张页面抓取失败，请记录序号，后续使用 3 菜单重新抓取\n",
      "正在抓取第[2]张页面\n",
      "第[2]张页面抓取失败，请记录序号，后续使用 3 菜单重新抓取\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8ccadb82539c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpage_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"请输入要抓取的页面数量（如要抓取100页，则输入100）：\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcookies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"_lxsdk_cuid=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _lxsdk=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _hc.v=ebb31d04-778f-02a6-9984-060d56d71344.1617504706; s_ViewType=10; ctu=3a87482384aa8445669918ae73680ee9a42215c28654ba91ce2d4ec9bf99ab35; ll=7fd06e815b796be3df069dec7836c3df; aburl=1; cityid=3; default_ab=index%3AA%3A3%7CshopList%3AA%3A5; switchcityflashtoast=1; fspop=test; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; Hm_lvt_602b80cf8079ae6591966cc70a3940e7=1618070722,1618320955,1618320969,1618322864; ua=dpuser_0539703749; uamo=17390601491; cy=16; cye=wuhan; Hm_lpvt_602b80cf8079ae6591966cc70a3940e7=1618335791; _lxsdk_s=178cc838e98-db3-7c3-be9%7C%7C1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mspider_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmenu_choose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# 解析html 文件 默认只解析文件大小大于 100KB 的文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8cbf54a4ea45>\u001b[0m in \u001b[0;36mspider_html\u001b[1;34m(url, page_num, cookie)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"第[\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcurrent_page_number\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"]张页面已保存在\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdir_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# 错误页面信息写入日志\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 抓取模块\n",
    "# import sprider_network\n",
    "# # 解析模块\n",
    "# import spider_parse\n",
    "\n",
    "# 把爬到的数据写到一个 Excel 表里。也就是 sheet 名为 \"湖锦\" 后续跟店名替换即可\n",
    "excel_sheet_name = \"湖锦\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    menu_choose = input(\"请输入菜单序号： 1. 爬页面  2. 解析 html 文件  3. 抓取未抓取到的页面: \")\n",
    "\n",
    "    if menu_choose == \"1\":\n",
    "        # 抓取页面\n",
    "        page_path = 'http://www.dianping.com/shop/l3P4II7t2LEV30qe/review_all/p'\n",
    "        page_num = input(\"请输入要抓取的页面数量（如要抓取100页，则输入100）：\")\n",
    "        cookies = \"_lxsdk_cuid=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _lxsdk=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _hc.v=ebb31d04-778f-02a6-9984-060d56d71344.1617504706; s_ViewType=10; ctu=3a87482384aa8445669918ae73680ee9a42215c28654ba91ce2d4ec9bf99ab35; ll=7fd06e815b796be3df069dec7836c3df; aburl=1; cityid=3; default_ab=index%3AA%3A3%7CshopList%3AA%3A5; switchcityflashtoast=1; fspop=test; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; Hm_lvt_602b80cf8079ae6591966cc70a3940e7=1618070722,1618320955,1618320969,1618322864; ua=dpuser_0539703749; uamo=17390601491; cy=16; cye=wuhan; Hm_lpvt_602b80cf8079ae6591966cc70a3940e7=1618335791; _lxsdk_s=178cc838e98-db3-7c3-be9%7C%7C1\"\n",
    "        spider_html(page_path, page_num, cookies)\n",
    "    elif menu_choose == \"2\":\n",
    "        # 解析html 文件 默认只解析文件大小大于 100KB 的文件\n",
    "        dir_path = 'D://360downloads//美团比赛//spider_result/'\n",
    "            # \"/Users/yunxuan/PycharmProjects/spider/spider_result/红鼎豆捞/sources\"\n",
    "        load_dir(dir_path)\n",
    "    else:\n",
    "        # 防止一次没抓完，有403/404 问题\n",
    "        page_path = input(\"请输入待抓取页面的 URL（只写到 p 前面的即可，如http://www.dianping.com/shop/k8tlcPTSvTPl1zUz/review_all/p）：\")\n",
    "        page_num = input(\"请输入要抓取的页码，以英文逗号分开，如：1,3,8：\")\n",
    "        cookies = \"_lxsdk_cuid=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _lxsdk=1789acb1ea1c8-0799a204fdb9ea-5c3f1e49-10ed98-1789acb1ea1c8; _hc.v=ebb31d04-778f-02a6-9984-060d56d71344.1617504706; s_ViewType=10; ctu=3a87482384aa8445669918ae73680ee9a42215c28654ba91ce2d4ec9bf99ab35; ll=7fd06e815b796be3df069dec7836c3df; aburl=1; cityid=3; default_ab=index%3AA%3A3%7CshopList%3AA%3A5; switchcityflashtoast=1; fspop=test; _lx_utm=utm_source%3DBaidu%26utm_medium%3Dorganic; Hm_lvt_602b80cf8079ae6591966cc70a3940e7=1618070722,1618320955,1618320969,1618322864; ua=dpuser_0539703749; uamo=17390601491; cy=16; cye=wuhan; Hm_lpvt_602b80cf8079ae6591966cc70a3940e7=1618335791; _lxsdk_s=178cc838e98-db3-7c3-be9%7C%7C1\"\n",
    "        page_number = page_num.split(\",\")\n",
    "        for i in range(0, len(page_number)):\n",
    "            spider_html(page_path, page_num, cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 浏览器标识\n",
    "from fake_useragent import UserAgent\n",
    "# 请求\n",
    "import requests\n",
    "# 文件读写\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 初始化一个浏览器标识对象\n",
    "ua = UserAgent()\n",
    "\n",
    "# 抓取页面的请求头信息\n",
    "request_pages_headers = {\n",
    "    'User-Agent': ua.random,\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Host\": \"www.dianping.com\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"max-age=0\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9,fr;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "}\n",
    "\n",
    "\n",
    "def spider_html(url, page_num, cookie):\n",
    "    \"\"\"\n",
    "    抓取 URL 页面\n",
    "    :param url: 待抓取的 URL 前缀，不含页码\n",
    "    :param page_num: 待抓取的页面数量\n",
    "    :param cookie: 登陆后的 COOKIE\n",
    "    :return: 没返回的\n",
    "    \"\"\"\n",
    "\n",
    "    # 给字典添加 Cookie\n",
    "    request_pages_headers[\"Cookie\"] = cookie\n",
    "    # 页码字符串转成数字\n",
    "    page_num_int = int(page_num)\n",
    "\n",
    "    # 不做过多判断了，毕竟不是商业化。只是方便使用的，不会随便传参数- -\n",
    "\n",
    "    # 定义一个错误页面数组\n",
    "    error_page = []\n",
    "\n",
    "    for i in range(0, page_num_int):\n",
    "        current_page_number = str(i + 1)\n",
    "        print(\"正在抓取第[\" + current_page_number + \"]张页面\")\n",
    "        # 拼装待抓取的 URL 全路径\n",
    "        request_url = url + str(i)\n",
    "        # 模拟打开浏览器，输入路径\n",
    "        html = requests.get(request_url, headers=request_pages_headers)\n",
    "        if html.status_code != 200:\n",
    "            print(\"第[\" + current_page_number + \"]张页面抓取失败，请记录序号，后续使用 3 菜单重新抓取\")\n",
    "            error_page.append(current_page_number)\n",
    "        else:\n",
    "            # 抓取成功的，写入文件\n",
    "            html_file_name = \"page(\" + current_page_number + \").html\"\n",
    "            # 声明一个目录\n",
    "            dir_path = os.path.abspath(\"spider_result\")\n",
    "            # 判断是否存在\n",
    "            is_exists = os.path.exists(dir_path)\n",
    "            if not is_exists:\n",
    "                # 不存在 创建目录\n",
    "                os.makedirs(dir_path)\n",
    "            # 存在 直接写入\n",
    "            with open(dir_path + \"/\" + html_file_name, \"w\", encoding=\"UTF-8\") as f:\n",
    "                f.write(html.text)\n",
    "                f.close()\n",
    "            print(\"第[\" + current_page_number + \"]张页面已保存在\" + dir_path)\n",
    "        time.sleep(3)\n",
    "\n",
    "    # 错误页面信息写入日志\n",
    "    dir_path = os.path.abspath(\"log\")\n",
    "    is_exists = os.path.exists(dir_path)\n",
    "    if not is_exists:\n",
    "        # 不存在 创建目录\n",
    "        os.makedirs(dir_path)\n",
    "    with open(dir_path + \"/error.log\", \"w\", encoding=\"UTF-8\") as f:\n",
    "        f.write(str(error_page))\n",
    "        f.close()\n",
    "    print(\"错误页面已保存至 error.log 文件中，路径：\", dir_path)\n",
    "    print(\"休息完毕，继续工作\")\n",
    "import os\n",
    "import xlwt\n",
    "import re\n",
    "import requests\n",
    "from pyquery import PyQuery as pq\n",
    "import datetime\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "\n",
    "ua = UserAgent()\n",
    "css_header = {\n",
    "    \"Accept\": \"text/k8tlcPTSvTPl1zUz,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    'Accept-Language': 'zh-CN,zh;q=0.9,fr;q=0.8',\n",
    "    'Connection': 'keep-alive',\n",
    "    \"Host\": \"s3plus.meituan.net\",\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': ua.random,\n",
    "}\n",
    "\n",
    "\n",
    "def load_dir(dir_path):\n",
    "    \"\"\"\n",
    "    加载目录\n",
    "    :param dir_path: html 文件存放目录\n",
    "    :return: 不返回了吧\n",
    "    \"\"\"\n",
    "    # 对目录下文件进行一个排序操作\n",
    "    file_paths = sorted(os.listdir(dir_path))\n",
    "    # 定义一下有多少个文件\n",
    "    file_count = len(file_paths)\n",
    "    # 只抓取2019-1.1 到 2021-1-1 即 2018-12-31 23:59 to 2021-01-01 00:00\n",
    "\n",
    "    # 定义一个 Excel 文件\n",
    "    workbook = xlwt.Workbook(encoding='utf-8')\n",
    "    #  添加一个工作表\n",
    "    worksheet = workbook.add_sheet(\"未命名工作表\")\n",
    "    # 0-10 第 0 行 0 - 10 列。 当做表名\n",
    "    worksheet.write(0, 0, label='评论时间')\n",
    "    worksheet.write(0, 1, label='用户名')\n",
    "    worksheet.write(0, 2, label='评论内容')\n",
    "    worksheet.write(0, 3, label='评论字数')\n",
    "    worksheet.write(0, 4, label='图片数量')\n",
    "    worksheet.write(0, 5, label='有无视频')\n",
    "    worksheet.write(0, 6, label='评分总分')\n",
    "    worksheet.write(0, 7, label='评论者等级')\n",
    "    worksheet.write(0, 8, label='评论下的评论数')\n",
    "    worksheet.write(0, 9, label='点赞数')\n",
    "    worksheet.write(0, 10, label='用户描述')\n",
    "    # 定义一个行数，用来写到 Excel 文件对应位置中\n",
    "    row_index = 1\n",
    "    for i in range(0, file_count):\n",
    "        if row_index == -1:\n",
    "            # 结束了，别解析了\n",
    "            break\n",
    "        # 解析文件\n",
    "        row_index = parse_html_file(dir_path + \"/\" + file_paths[i], worksheet, row_index)\n",
    "\n",
    "    workbook.save(dir_path + \"/result.xls\")\n",
    "\n",
    "\n",
    "def parse_html_file(file_path, worksheet, row_index):\n",
    "    \"\"\"\n",
    "    解析 html 文件\n",
    "    :param file_path: 文件路径\n",
    "    :param worksheet: Excel 文件对象\n",
    "    :param row_index: Excel 的行号 从第 1 行开始，向下写\n",
    "    :return:\n",
    "        row_index: 是否结束解析标志 -1 结束解析\n",
    "    \"\"\"\n",
    "    # 读取文件内容到ret 变量中\n",
    "    ret = open(file_path).read()\n",
    "    # 使用选择器把文本转成对象\n",
    "    doc = pq(ret)\n",
    "\n",
    "    # 找页面中的评论区\n",
    "    comments_area = doc(\"div.reviews-items > ul > li\").items()\n",
    "\n",
    "    for data in comments_area:\n",
    "        # 解析 css\n",
    "        dict_svg_text, list_svg_y, dict_css_x_y, is_continue = parse_css(ret)\n",
    "\n",
    "        if not is_continue:\n",
    "            # css 样式解析失败，没必要再解析html 页面了。解析出来也是错误的\n",
    "            print(\"加密 css 样式解析失败：\", file_path)\n",
    "            return row_index\n",
    "        # 用户名\n",
    "        user_name = data(\"div.main-review > div.dper-info > a\").text()\n",
    "        # 用户评分星级[10-50]\n",
    "        start_shop = str(data(\"div.review-rank > span\").attr(\"class\")).split(\" \")[1].replace(\"sml-str\", \"\")\n",
    "        # 用户描述：机器：非常好 环境：非常好 服务：非常好 人均：0元\n",
    "        describe_shop = data(\"div.review-rank > span.score\").text()\n",
    "        # 关键部分，评论HTML,待处理，评论包含隐藏部分和直接展示部分，默认从隐藏部分获取数据，没有则取默认部分。（查看更多）\n",
    "        comments = data(\"div.review-words.Hide\").html()\n",
    "        try:\n",
    "            len(comments)\n",
    "        except:\n",
    "            # 展开评价\n",
    "            comments = data(\"div.review-words\").html()\n",
    "        # 图片数量\n",
    "        pictures = data(\"div.main-review > div.review-pictures > ul > li > a > img\")\n",
    "        # 数量\n",
    "        pic_num = pictures.length\n",
    "        pic_link = str(pic_num) + \"张：\\n\"\n",
    "        # 链接\n",
    "        for pic in pictures.items():\n",
    "            pic_link += (str(pic.attr('data-big')) + \"\\n\")\n",
    "        # 评论点赞数\n",
    "        comments_click_goods_num_wrapper = data(\"div.main-review > div.misc-info.clearfix > span.actions\")\n",
    "        comments_click_goods_number_wrapper_children = comments_click_goods_num_wrapper.children()\n",
    "        comments_goods_num = comments_click_goods_number_wrapper_children.text().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        # 发表评论的时间\n",
    "        comments_time = data(\"div.main-review > div.misc-info.clearfix > span.time\").text()\n",
    "        if \"更新于\" in comments_time:\n",
    "            comments_time_array = comments_time.split('更新于')\n",
    "            date_string = comments_time_array[0].replace(\"\\xa0\", \"\").strip()\n",
    "            comments_time_date_time_type = datetime.datetime.strptime(date_string, '%Y-%m-%d')\n",
    "            # 结束时间 2019-01-01\n",
    "            end_time = datetime.datetime.strptime(\"2018-12-31 23:59\", \"%Y-%m-%d %H:%M\")\n",
    "            # 日期比较  如果到达这个时间。 则不继续爬\n",
    "            diff = comments_time_date_time_type - end_time\n",
    "            if diff.days <= 0:\n",
    "                # 说明是2018 年 12 月 31 日前的数据\n",
    "                return -1\n",
    "        else:\n",
    "            # 评论时间转日期类型\n",
    "            comments_time_date_time_type = datetime.datetime.strptime(comments_time, '%Y-%m-%d %H:%M')\n",
    "            # 结束时间 2019-01-01\n",
    "            end_time = datetime.datetime.strptime(\"2018-12-31 23:59\", \"%Y-%m-%d %H:%M\")\n",
    "            # 日期比较  如果到达这个时间。 则不继续爬\n",
    "            diff = comments_time_date_time_type - end_time\n",
    "            if diff.days <= 0:\n",
    "                # 说明是2018 年 12 月 31 日前的数据\n",
    "                return -1\n",
    "\n",
    "        # 评论内容 根据上边的字典，去对应评论区的文字\n",
    "        comments_content = css_decode(dict_css_x_y, dict_svg_text, list_svg_y, comments)\n",
    "        print(\"评论内容：\", comments_content)\n",
    "        worksheet.write(row_index, 0, label=comments_time)\n",
    "        worksheet.write(row_index, 1, label=user_name)\n",
    "        worksheet.write(row_index, 2, label=comments_content)\n",
    "        worksheet.write(row_index, 3, label=len(comments_content))\n",
    "        worksheet.write(row_index, 4, label=pic_link)\n",
    "        worksheet.write(row_index, 5, label=\"1\")\n",
    "        worksheet.write(row_index, 6, label=start_shop)\n",
    "        worksheet.write(row_index, 7, label=\"1\")\n",
    "        worksheet.write(row_index, 8, label=\"见点赞数中的信息\")\n",
    "        worksheet.write(row_index, 9, label=comments_goods_num)\n",
    "        worksheet.write(row_index, 10, label=describe_shop)\n",
    "        row_index += 1\n",
    "    return row_index\n",
    "\n",
    "\n",
    "def parse_css(ret):\n",
    "    \"\"\"\n",
    "    解析css的方法 主要用来处理评论区中的 css 样式 以及处理加密字体\n",
    "    注意：这里大众点评有目前有两种形式，所以我们要针对两种不同的方式使用不同的解析方式处理：\n",
    "    一种 svg 数据\n",
    "        以 <path id=\"xxx\" d=\"xx xxx xxx\" /> 为键 \n",
    "        以 <textPath xlink:href=\"xx\" textLength=\"xx\">xxxx</textPath>为value 的形式存储明文\n",
    "    我们定义为 A 方式\n",
    "    另外一种直接\n",
    "        以 <text x=\"xxx\" y=\"xxx\">xxx</text>的形式存储\n",
    "    我们定义为 B 方式\n",
    "    :param ret: 网页源代码\n",
    "    :return: \n",
    "    dict_svg_text：key - value 形式返回 svg 数据\n",
    "    list_svg_y：\n",
    "        A 方式返回 <path>标签里的[x,y]坐标轴，以[x,y]形式返回\n",
    "        B 方式返回 <text>标签里的y数据，x 以自增的形式组装成[x,y]的形式\n",
    "    dict_css_x_y：css 样式中，每个加密字体的密文 形如：<svgmtsi class=\"xxxx\"></svgmtsi>之类的\n",
    "        根据class 找到对应样式中的background: apx bpx 中的[a,b]\n",
    "    is_continue:\n",
    "        是否继续的标志\n",
    "    \"\"\"\n",
    "    # 定义一个是否继续的标识，True 代表继续 False 代表不继续\n",
    "    is_continue = True\n",
    "    # 从当前页面中找到第一个包含 svgtextcss 关键字的 css 路径  目前来说，页面一般只有一个\n",
    "    # <link style=\"text/css\" href=\"//xxxxx/svgtextcss/xxxx.css\"/>\n",
    "    css_path_obj = re.search(r'<link re.*?css.*?href=\\\"(.*/svgtextcss/.*?)\\\"', ret)\n",
    "    try:\n",
    "        # 组装完整 url 这个1 代表上述正则中的第二个 (.*?)内容 即 (.*/svgtextcss/.*?)\n",
    "        css_link = \"http:\" + str(css_path_obj[1])\n",
    "    except:\n",
    "        # 没找到。为啥没找到？说明页面中没有，为啥没有。说明页面抓取失败- -\n",
    "        is_continue = False\n",
    "        return None, None, None, is_continue\n",
    "    print(\"获取 CSS 样式中...\", css_link)\n",
    "    # css 文件的内容 通篇 .className{ background: apx bpx; } 其中有三个带 url(//) 的\n",
    "    css_html = requests.get(css_link, headers=css_header)\n",
    "    # svg 加密文字的 URL 路径，目前来说，一个 css 文件中有3 个。经测试，一般第二个是我们使用的，第一个跟第三个我也不知道干啥的，\n",
    "    # 第一个打开是一串数字，大概9 位\n",
    "    # 第三个打开是一小部分明文，但是感觉不太像，跟着感觉走～\n",
    "    svg_link_array = re.compile(\"\\/\\/(.*?)\\)\").findall(str(css_html.text))\n",
    "    # 解析 svg 文本内容\n",
    "    dict_avg_text, list_svg_y = parse_svg_text(svg_link_array)\n",
    "    # 解析 css 样式表，取background 中 a,b 组装字典\n",
    "    dict_css_x_y = parse_css_text(css_html.text)\n",
    "    return dict_avg_text, list_svg_y, dict_css_x_y, is_continue\n",
    "\n",
    "\n",
    "def parse_svg_text(svg_link_array):\n",
    "    \"\"\"\n",
    "    解析加密字典的文件\n",
    "    :param svg_link_array: 文件的 URL 数组\n",
    "    :return:\n",
    "      dict_svg：组装的字典\n",
    "      list_y：\n",
    "    \"\"\"\n",
    "    length = 0\n",
    "    svg_html_text = ''\n",
    "    for i in range(0, len(svg_link_array)):\n",
    "        # 请求连接\n",
    "        svg_html = requests.get(\"http://\" + svg_link_array[i], headers=css_header)\n",
    "        content_length = int(svg_html.headers[\"Content-Length\"])\n",
    "        if content_length > length:\n",
    "            length = content_length\n",
    "            svg_html_text = svg_html.text\n",
    "\n",
    "    dict_svg, list_y = parse_svg_test(svg_html_text)\n",
    "    return dict_svg, list_y\n",
    "\n",
    "\n",
    "def parse_svg_test(svg_html_text):\n",
    "    svg_text_r = r'<textPath xlink:href=\"(.*?)\" textLength=\"(.*?)\">(.*?)</textPath>'\n",
    "    svg_text_re = re.findall(svg_text_r, svg_html_text)\n",
    "    if len(svg_text_re) == 0:\n",
    "        return parse_svg_test_b(svg_html_text)\n",
    "    else:\n",
    "        return parse_svg_test_a(svg_html_text)\n",
    "\n",
    "\n",
    "# A 方式\n",
    "def parse_svg_test_a(svg_html):\n",
    "    \"\"\"\n",
    "    形如：\n",
    "        http://s3plus.meituan.net/v1/mss_0a06a471f9514fc79c981b5466f56b91/svgtextcss/74d63812e5b327d850ab4a8782833d47.svg\n",
    "    页面上有两部分内容\n",
    "    一部分是位于上方的 <defs> 下的\n",
    "        <path id=\"(.*?)\" d=\"(.*?) (.*?) (.*?)\"/>\n",
    "    一部分是位于下方的 <text lengthAdjust=\"spacing\"> 下的\n",
    "        <textPath xlink:href=\"(.*?)\" textLength=\"(.*?)\">(.*?)</textPath>\n",
    "\n",
    "    <defs>\n",
    "        <path xmlns=\"http://www.w3.org/2000/svg\" id=\"32\" d=\"M0 1317 H600\"/>\n",
    "        <path xmlns=\"http://www.w3.org/2000/svg\" id=\"33\" d=\"M0 1364 H600\"/>\n",
    "    </defs>\n",
    "    <text lengthAdjust=\"spacing\">\n",
    "        <textPath xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#32\" textLength=\"336\">枪畜栏督胖扶遍秒搞笨类敏蛛与诊绵病蓝份碑往气焰望</textPath>\n",
    "        <textPath xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#33\" textLength=\"476\">腥钉息元易分绝归当洽疑桂畅朵照仍船从论织朗瓣讲首此苗砌能您泛押蜜徐膏</textPath>\n",
    "        <textPath xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#34\" textLength=\"378\">数幼段请攻议耐仙锻征兼句拼愁义食校楚商姻映辆黎鸽怕被测</textPath>\n",
    "    </text>\n",
    "    我们要根据 path 标签里 d 的第二位，对应 css 样式表中 background 的bpx 参数\n",
    "    假设有一个 css .ory4hj{background:-168.0px -1353.0px;}\n",
    "    其中 a = -168.0  b = -1353.0\n",
    "    全部取正 1317 < b < 1364 则我们要的字在 1364 的 id 33 中\n",
    "    33 对应 <textPath> 中的 xlink:href=\"#33\"\n",
    "    :param svg_html: 源文件\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    svg_text_r = r'<textPath xlink:href=\"(.*?)\" textLength=\"(.*?)\">(.*?)</textPath>'\n",
    "    svg_text_re = re.findall(svg_text_r, svg_html)\n",
    "    dict_avg = {}\n",
    "    i = 0\n",
    "    # 生成svg加密字体库字典\n",
    "    for data in svg_text_re:\n",
    "        dict_avg[i] = list(data[2])\n",
    "        # print(\"生成解密字典\", i, \":\", dict_avg[i])\n",
    "        i += 1\n",
    "    svg_y_r = r'<path id=\"(.*?)\" d=\"(.*?) (.*?) (.*?)\"/>'\n",
    "    svg_y_re = re.findall(svg_y_r, svg_html)\n",
    "    list_y = []\n",
    "    # 存储('18', 'M0', '748', 'H600') eg:(x坐标，未知，y坐标，未知)\n",
    "    for data in svg_y_re:\n",
    "        data_ = [int(data[0]) - 1, data[2]]\n",
    "        list_y.append(data_)\n",
    "        # print(\"存储解密索引\", \"行号：\", data_, \"行索引\", data[2])\n",
    "    return dict_avg, list_y\n",
    "\n",
    "\n",
    "# B 方式\n",
    "def parse_svg_test_b(svg_html):\n",
    "    \"\"\"\n",
    "    http://s3plus.meituan.net/v1/mss_0a06a471f9514fc79c981b5466f56b91/svgtextcss/ca95f77abfb8871e1fb351905d6f3239.svg\n",
    "    同理。解密思路不变。只是找数据的方式变了。参考方式 A\n",
    "    <text xmlns=\"http://www.w3.org/2000/svg\" x=\"0\" y=\"805\">莲陡利槽供二本松边晶叹帖聪倡夕蝶喇夫拴涌爹激驳醋焦泥届萄痒送链警店柄乒训能胸露嗓咳般</text>\n",
    "    :param svg_html:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    svg_text_r = r'<text x=\"(.*?)\" y=\"(.*?)\">(.*?)</text>'\n",
    "    svg_text_re = re.findall(svg_text_r, svg_html)\n",
    "    dict_avg = {}\n",
    "\n",
    "    i = 0\n",
    "    # 生成svg加密字体库字典\n",
    "    for data in svg_text_re:\n",
    "        dict_avg[i] = list(data[2])\n",
    "        i += 1\n",
    "    svg_y_r = r'<text x=\"(.*?)\" y=\"(.*?)\">(.*?)</text>'\n",
    "    svg_y_re = re.findall(svg_y_r, svg_html)\n",
    "    list_y = []\n",
    "    j = 0\n",
    "    for data in svg_y_re:\n",
    "        list_y.append([j, data[1]])\n",
    "        j += 1\n",
    "    return dict_avg, list_y\n",
    "\n",
    "\n",
    "def parse_css_text(css_html):\n",
    "    \"\"\"\n",
    "    http://s3plus.meituan.net/v1/mss_0a06a471f9514fc79c981b5466f56b91/svgtextcss/883f229cc13ecde07cc3e3e3af16819e.css\n",
    "    这个就比较简单了。取.ory4hj{background:-168.0px -1353.0px;} a 值 b 值就行\n",
    "    :param css_html: css 源文件\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    css_text_r = r'.(.*?){background:(.*?)px (.*?)px;}'\n",
    "    css_text_re = re.findall(css_text_r, css_html)\n",
    "    dict_css = {}\n",
    "    for data in css_text_re:\n",
    "        \"\"\"\n",
    "        加密字库.ory4hj{background:-168.0px -1353.0px;} 与svg文件对应关系，x/14，根据font-size 计算\n",
    "        y，原样返回，需要在svg函数中做处理\n",
    "        \"\"\"\n",
    "        x = int(float(data[1]) / -14)\n",
    "        \"\"\"\n",
    "        字典参数：{css参数名：(background-x,background-y,background-x/14,background-y)}\n",
    "        \"\"\"\n",
    "        dict_css[data[0]] = (data[1], data[2], x, data[2])\n",
    "        # print(\"存储加密解析数据\", \"索引值：\", data[0], \"x 向量: \", data[1], \"y 向量：\", data[2], \"加密字符位置：\", x)\n",
    "    return dict_css\n",
    "\n",
    "\n",
    "def css_decode(css_html, svg_dictionary, svg_list, comments_html):\n",
    "    \"\"\"\n",
    "    最终评论汇总\n",
    "    :param css_html: css 的HTML源码\n",
    "    :param svg_dictionary: svg加密字库的字典\n",
    "    :param svg_list: svg加密字库对应的坐标数组[x, y]\n",
    "    :param comments_html: 评论的HTML源码，对应0-详情页的评论，在此处理\n",
    "    :return: 最终合成的评论\n",
    "    \"\"\"\n",
    "    css_dict_text = css_html\n",
    "    csv_dict_text, csv_dict_list = svg_dictionary, svg_list\n",
    "    # 处理评论源码中的 svgmtsi 标签，生成字典key\n",
    "    comments_text = comments_html.replace('<svgmtsi class=\"', ',').replace('\"/>', \",\").replace('\">', \",\")\n",
    "    comments_list = [x for x in comments_text.split(\",\") if x != '']\n",
    "    comments_str = []\n",
    "    for msg in comments_list:\n",
    "        # 如果有加密标签\n",
    "        if msg in css_dict_text:\n",
    "            # 参数说明：[x,y] css样式中background 的[x/14，y]\n",
    "            x = int(css_dict_text[msg][2])\n",
    "            y = -float(css_dict_text[msg][3])\n",
    "            # 寻找background的y轴比svg<path>标签里的y轴小的第一个值对应的坐标就是<textPath>的href值\n",
    "            for g in csv_dict_list:\n",
    "                if y < int(g[1]):\n",
    "                    comments_str.append(csv_dict_text[int(g[0])][x])\n",
    "                    break\n",
    "        # 没有加密标签\n",
    "        else:\n",
    "            comments_str.append(msg.replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "    str_comments = \"\"\n",
    "    for x in comments_str:\n",
    "        str_comments += x\n",
    "    # 处理特殊标签\n",
    "    dr = re.compile(r'</?\\w+[^>]*>', re.S)\n",
    "    dr2 = re.compile(r'<img+[^;]*', re.S)\n",
    "    dr3 = re.compile(r'&(.*?);', re.S)\n",
    "    dd = dr.sub('', str_comments)\n",
    "    dd2 = dr2.sub('', dd)\n",
    "    comments_str = dr3.sub('', dd2)\n",
    "    return comments_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
